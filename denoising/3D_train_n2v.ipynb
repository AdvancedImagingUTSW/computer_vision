{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Noise2Void\n",
    "Project cloned from: https://github.com/juglab/n2v\n",
    "\n",
    "$ git clone https://github.com/juglab/n2v.git\n",
    "\n",
    "Change into its directory and install it:\n",
    "\n",
    "$ pip install -e .\n",
    "\n",
    "During operation it can be useful to evaluate GPU usage.  To do so, open a terminal and execute the following command:\n",
    "watch -n 0.5 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kdean/.conda/envs/n2venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home2/kdean/.conda/envs/n2venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home2/kdean/.conda/envs/n2venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home2/kdean/.conda/envs/n2venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home2/kdean/.conda/envs/n2venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home2/kdean/.conda/envs/n2venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from n2v.models import N2VConfig, N2V\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "import numpy as np\n",
    "from tifffile import imread\n",
    "from csbdeep.io import save_tiff_imagej_compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation\n",
    "\n",
    "* Create directory for saving data.\n",
    "* Create DataGenerator-object - helps load data and extract patches for training and validation.\n",
    "* Load all .tif files - Important to know how the data is loaded, and you can check this by printing the shape.  In some cases, OME-XML for some reason loads all of the volumes from the data.\n",
    "* Display shape of the images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 920, 1172)\n"
     ]
    }
   ],
   "source": [
    "# Specify where the data is located\n",
    "data_directory = '/project/shared/dean_wbmf/denoise_attempt'\n",
    "image_name = 'ch02_subset.tif'\n",
    "test = imread(os.path.join(data_directory,image_name))\n",
    "print(test.shape)\n",
    "\n",
    "# Create a folder for the results\n",
    "if not os.path.isdir(os.path.join(data_directory,'denoised')):\n",
    "    os.mkdir(os.path.join(data_directory,'denoised'))\n",
    "\n",
    "# Create Data Generator\n",
    "datagen = N2V_DataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data does not trigger the OME-XML import, you can load each image from the directory as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = datagen.load_imgs_from_directory(directory=data_directory, dims='TZYX')\n",
    "print(imgs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data does trigger OME-XML import, you can load only the first image from the directory, which will then automatically load the rest too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(data_directory,image_name)]\n",
    "imgs = datagen.load_imgs(files, dims='TZYX')\n",
    "print(imgs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150, 920, 1172, 1)\n"
     ]
    }
   ],
   "source": [
    "files = [os.path.join(data_directory,image_name)]\n",
    "imgs = datagen.load_imgs(files, dims='ZYX')\n",
    "print(imgs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Patches\n",
    "* default = 32,64,64\n",
    "* training images must be evenly divisible by 4 along axes XYZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = datagen.generate_patches_from_list(imgs[:1], shape=(32, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "numberEpochs = 40\n",
    "\n",
    "# default = :600\n",
    "X = patches[:600]\n",
    "X_val = patches[600:]\n",
    "config = N2VConfig(X, unet_kern_size=3, train_steps_per_epoch=int(X.shape[0]/128),\n",
    "                   train_epochs=numberEpochs,train_loss='mse', batch_norm=True, train_batch_size=4,\n",
    "                   n2v_perc_pix=0.198, n2v_patch_shape=(32, 64, 64), n2v_manipulator='uniform_withCP',\n",
    "                   n2v_neighborhood_radius=5)\n",
    "vars(config)\n",
    "model_name = str(numberEpochs) + 'epochs'\n",
    "model = N2V(config=config, name=model_name, basedir=data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run N2V Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 blind-spots will be generated per training patch of size (32, 64, 64).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing validation data: 100%|██████████| 7464/7464 [00:04<00:00, 1637.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "4/4 [==============================] - 1225s 306s/step - loss: 1.0262 - n2v_mse: 1.0262 - n2v_abs: 0.8215 - val_loss: 1.1113 - val_n2v_mse: 1.1113 - val_n2v_abs: 0.7742\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - 1223s 306s/step - loss: 0.9357 - n2v_mse: 0.9357 - n2v_abs: 0.7909 - val_loss: 1.2679 - val_n2v_mse: 1.2679 - val_n2v_abs: 0.7890\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.8551 - n2v_mse: 0.8551 - n2v_abs: 0.7399 - val_loss: 2.5527 - val_n2v_mse: 2.5527 - val_n2v_abs: 1.1459\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.4578 - n2v_mse: 0.4578 - n2v_abs: 0.5513 - val_loss: 2.4181 - val_n2v_mse: 2.4181 - val_n2v_abs: 1.0463\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - 1225s 306s/step - loss: 0.4953 - n2v_mse: 0.4953 - n2v_abs: 0.5470 - val_loss: 2.3130 - val_n2v_mse: 2.3130 - val_n2v_abs: 1.0403\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.3821 - n2v_mse: 0.3821 - n2v_abs: 0.4817 - val_loss: 1.8160 - val_n2v_mse: 1.8160 - val_n2v_abs: 0.9742\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - 1225s 306s/step - loss: 0.2900 - n2v_mse: 0.2900 - n2v_abs: 0.4189 - val_loss: 1.2134 - val_n2v_mse: 1.2134 - val_n2v_abs: 0.8115\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.4051 - n2v_mse: 0.4051 - n2v_abs: 0.4971 - val_loss: 1.2033 - val_n2v_mse: 1.2033 - val_n2v_abs: 0.8025\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.4833 - n2v_mse: 0.4833 - n2v_abs: 0.5797 - val_loss: 1.3027 - val_n2v_mse: 1.3027 - val_n2v_abs: 0.8149\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.2790 - n2v_mse: 0.2790 - n2v_abs: 0.4224 - val_loss: 0.9458 - val_n2v_mse: 0.9458 - val_n2v_abs: 0.7173\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - 1225s 306s/step - loss: 0.3608 - n2v_mse: 0.3608 - n2v_abs: 0.4624 - val_loss: 3.0769 - val_n2v_mse: 3.0769 - val_n2v_abs: 1.1455\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.2552 - n2v_mse: 0.2552 - n2v_abs: 0.4090 - val_loss: 3.1978 - val_n2v_mse: 3.1978 - val_n2v_abs: 1.1507\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - 1225s 306s/step - loss: 0.2892 - n2v_mse: 0.2892 - n2v_abs: 0.4361 - val_loss: 1.6057 - val_n2v_mse: 1.6057 - val_n2v_abs: 0.8737\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - 1224s 306s/step - loss: 0.5093 - n2v_mse: 0.5093 - n2v_abs: 0.5657 - val_loss: 1.1412 - val_n2v_mse: 1.1412 - val_n2v_abs: 0.6593\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - 1235s 309s/step - loss: 0.2269 - n2v_mse: 0.2269 - n2v_abs: 0.3494 - val_loss: 0.8311 - val_n2v_mse: 0.8311 - val_n2v_abs: 0.6191\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - 1309s 327s/step - loss: 0.2048 - n2v_mse: 0.2048 - n2v_abs: 0.3626 - val_loss: 2.3429 - val_n2v_mse: 2.3429 - val_n2v_abs: 1.0285\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - 2050s 513s/step - loss: 0.2213 - n2v_mse: 0.2213 - n2v_abs: 0.3843 - val_loss: 2.4700 - val_n2v_mse: 2.4700 - val_n2v_abs: 1.0479\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - 2386s 596s/step - loss: 0.2222 - n2v_mse: 0.2222 - n2v_abs: 0.3578 - val_loss: 1.0526 - val_n2v_mse: 1.0526 - val_n2v_abs: 0.7052\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - 2376s 594s/step - loss: 0.2018 - n2v_mse: 0.2018 - n2v_abs: 0.3534 - val_loss: 0.3782 - val_n2v_mse: 0.3782 - val_n2v_abs: 0.4559\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - 2392s 598s/step - loss: 0.2842 - n2v_mse: 0.2842 - n2v_abs: 0.4271 - val_loss: 0.4318 - val_n2v_mse: 0.4318 - val_n2v_abs: 0.5049\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - 2395s 599s/step - loss: 0.2245 - n2v_mse: 0.2245 - n2v_abs: 0.3725 - val_loss: 0.5329 - val_n2v_mse: 0.5329 - val_n2v_abs: 0.5621\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - 2377s 594s/step - loss: 0.2141 - n2v_mse: 0.2141 - n2v_abs: 0.3737 - val_loss: 0.3974 - val_n2v_mse: 0.3974 - val_n2v_abs: 0.4830\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - 2391s 598s/step - loss: 0.1783 - n2v_mse: 0.1783 - n2v_abs: 0.3380 - val_loss: 0.3047 - val_n2v_mse: 0.3047 - val_n2v_abs: 0.4212\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - 2373s 593s/step - loss: 0.1337 - n2v_mse: 0.1337 - n2v_abs: 0.2946 - val_loss: 0.2629 - val_n2v_mse: 0.2629 - val_n2v_abs: 0.3896\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - 2387s 597s/step - loss: 0.2176 - n2v_mse: 0.2176 - n2v_abs: 0.3732 - val_loss: 0.2714 - val_n2v_mse: 0.2714 - val_n2v_abs: 0.3932\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - 2374s 594s/step - loss: 0.2102 - n2v_mse: 0.2102 - n2v_abs: 0.3602 - val_loss: 0.2844 - val_n2v_mse: 0.2844 - val_n2v_abs: 0.3954\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - 2391s 598s/step - loss: 0.1365 - n2v_mse: 0.1365 - n2v_abs: 0.2919 - val_loss: 0.3649 - val_n2v_mse: 0.3649 - val_n2v_abs: 0.4408\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - 2389s 597s/step - loss: 0.1781 - n2v_mse: 0.1781 - n2v_abs: 0.3370 - val_loss: 0.4603 - val_n2v_mse: 0.4603 - val_n2v_abs: 0.4796\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - 2376s 594s/step - loss: 0.1670 - n2v_mse: 0.1670 - n2v_abs: 0.3215 - val_loss: 0.3954 - val_n2v_mse: 0.3954 - val_n2v_abs: 0.4477\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - 2390s 598s/step - loss: 0.1810 - n2v_mse: 0.1810 - n2v_abs: 0.3300 - val_loss: 0.2435 - val_n2v_mse: 0.2435 - val_n2v_abs: 0.3763\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - 2346s 586s/step - loss: 0.2048 - n2v_mse: 0.2048 - n2v_abs: 0.3662 - val_loss: 0.2791 - val_n2v_mse: 0.2791 - val_n2v_abs: 0.3961\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - 2276s 569s/step - loss: 0.1911 - n2v_mse: 0.1911 - n2v_abs: 0.3504 - val_loss: 0.2315 - val_n2v_mse: 0.2315 - val_n2v_abs: 0.3683\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - 2277s 569s/step - loss: 0.2049 - n2v_mse: 0.2049 - n2v_abs: 0.3611 - val_loss: 0.1960 - val_n2v_mse: 0.1960 - val_n2v_abs: 0.3400\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - 2320s 580s/step - loss: 0.2748 - n2v_mse: 0.2748 - n2v_abs: 0.4166 - val_loss: 0.1775 - val_n2v_mse: 0.1775 - val_n2v_abs: 0.3253\n",
      "Epoch 35/40\n",
      "3/4 [=====================>........] - ETA: 9s - loss: 0.2235 - n2v_mse: 0.2235 - n2v_abs: 0.3960 "
     ]
    }
   ],
   "source": [
    "history = model.train(X, X_val)\n",
    "print(sorted(list(history.history.keys())))\n",
    "model.export_TF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Training and Export a Denoised Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150, 920, 1172, 1)\n",
      "Loading network weights from 'weights_best.h5'.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d6a2af1a2f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the image, and predict the denoised image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZYX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_tiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msave_tiff_imagej_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'denoised'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'denoised.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ZYX'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/GIT/external/n2v/n2v/models/n2v_standard.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, img, axes, resizer, n_tiles)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mrestored\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "files = [os.path.join(data_directory,image_name)]\n",
    "imgs = datagen.load_imgs(files, dims='ZYX')\n",
    "print(imgs[0].shape)\n",
    "\n",
    "model_name = str(40) + 'epochs'\n",
    "model = N2V(config=None, name=model_name, basedir=os.path.join(data_directory))\n",
    "\n",
    "# Load the image, and predict the denoised image.\n",
    "pred = model.predict(imgs, axes='ZYX', n_tiles=(2,4,4))\n",
    "save_tiff_imagej_compatible(os.path.join(data_directory, 'denoised','denoised.tif'), pred, 'ZYX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
